[directories]
#directory where the training data will be retrieved
#train_data = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/train_si84_multi
#train_data = /home/gkb/Download/kaldi/egs/yesno/s5/data/train_yesno
train_data = /home/gkb/Download/kaldi/egs/timit/s5/data/train

#directory where the testing data will be retrieved
#test_data = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/test_eval92
#test_data = /home/gkb/Download/kaldi/egs/yesno/s5/data/test_yesno
test_data = /home/gkb/Download/kaldi/egs/timit/s5/data/test

#directory where the training features will be stored and retrieved
#train_features = /esat/spchtemp/scratch/vrenkens/tfkaldi/AURORA4/features/train
#train_features = /home/gkb/Download/kaldi/egs/yesno/s5/data/train_yesno
train_features = /home/gkb/Download/kaldi/egs/timit/s5/data/train

#directory where the testing features will be stored and retrieved
#test_features = /esat/spchtemp/scratch/vrenkens/tfkaldi/AURORA4/features/test
#test_features = /home/gkb/Download/kaldi/egs/yesno/s5/data/test_yesno
test_features =/home/gkb/Download/kaldi/egs/timit/s5/data/test

#directory where the language model will be retrieved
#language = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/lang
#language = /home/gkb/Download/kaldi/egs/yesno/s5/data/lang
language = /home/gkb/Download/kaldi/egs/timit/s5/data/lang

#directory where the language model will be retrieved that is used to create the decoding graph
#language_test = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/lang_test_tgpr_5k
#language_test = /home/gkb/Download/kaldi/egs/yesno/s5/data/lang_test_tg
language_test = /home/gkb/Download/kaldi/egs/timit/s5/data/lang_test_bg

#directory where the all the data from this experiment will be stored (logs, models, ...)
#expdir = /esat/spchtemp/scratch/vrenkens/tfkaldi/AURORA4/expdir
#expdir = /home/gkb/Download/kaldi/egs/yesno/s5/exp/mono0a
expdir = /home/gkb/Download/kaldi/egs/timit/s5/exp

#path to the kaldi egs folder
#kaldi_egs = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5
#kaldi_egs = /home/gkb/Download/kaldi/egs/yesno/s5
kaldi_egs = /home/gkb/Download/kaldi/egs/timit/s5


[general]
# number of jobs for kaldi
# num_jobs = 8
#num_jobs = 1
train_num_jobs = 30
decode_num_jobs = 5
#command used for kaldi
#cmd = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5/utils/run.pl
cmd = /home/gkb/Download/kaldi/egs/wsj/s5/utils/run.pl

[gmm-features]
#name of the features
name = 13mfcc
#name
#feature type options: mfcc, fbank and ssc
type = mfcc
#the dynamic information that is added to the features, options are nodelta, delta and ddelta
dynamic = nodelta
#length of the sliding window (seconds)
winlen = 0.025
#step of the sliding window (seconds)
winstep = 0.01
#number of fbank filters
nfilt = 23
#number of fft bins
nfft = 512
#low cuttof frequency
lowfreq = 0
#hight cutoff frequency, if -1 set to None
highfreq = -1
#premphesis
preemph = 0.97
#include energy in features
include_energy = False
#snip the edges for sliding window
snip_edges = True
#mfcc option: number of cepstrals
numcep = 13
#mfcc option: cepstral lifter (used to scale the mfccs)
ceplifter = 22

[dnn-features]
#name of the features. If you want to use the GMM features, give it the same name
name = 40fbank
#feature type options: mfcc, fbank and ssc
type = fbank
#the dynamic information that is added to the features, options are nodelta, delta and ddelta
dynamic = nodelta
#length of the sliding window (seconds)
winlen = 0.025
#step of the sliding window (seconds)
winstep = 0.01
#number of fbank filters
nfilt = 40
#number of fft bins
nfft = 512
#low cuttof frequency
lowfreq = 0
#hight cutoff frequency, if -1 set to None
highfreq = -1
#premphesis
preemph = 0.97
#include energy in features
include_energy = False
#snip the edges for sliding window
snip_edges = True

[mono_gmm]
#name of the monophone gmm
boost_silence = 1.25
name = mono_gmm
tot_gauss = 1000

[tri_gmm]
#name of the triphone gmm
name = tri_gmm
#triphone gmm parameters (kaldi)
num_leaves = 2500
tot_gauss = 15000

[lda_mllt]
#name of the LDA+MLLT GMM
name = lda_mllt_gmm
#size of the left and right context window
#context_width = 3
context_ops = --splice-opts "--left-context=3 --right-context=3"
use_graphs = true
#lda_mllt gmm parameters (kaldi)
num_leaves = 2500
tot_gauss = 15000

[lda_mllt_sat]
name = lda_mlt_sat_gmm
num_leaves = 2500
tot_gauss = 15000

[ubm]
name = ubm4
tot_gauss = 400

[sgmm2]
name = sgmm2_4
use_graphs = true
use_gselect = true
num_leaves = 7000
tot_gauss = 9000

[nnet]
#name of the neural net
name = 300_2_tanh_batchnorm

#name of the gmm model used for the alignments
gmm_name = lda_mlt_sat_gmm

#size of the left and right context window
context_width = 5

#number of neurons in the hidden layers
num_hidden_units = 300

#number of hidden layers
num_hidden_layers = 2

#the network is initialized layer by layer. This parameters determines the frequency of adding layers. Adding will stop when the total number of layers is reached. Set to 0 if no layer-wise initialisation is required
add_layer_period = 0

#starting step, set to 'final' to skip nnet training
starting_step = 0

#if you're using monophone alignments, set to True
monophone = False

#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = tanh

#if you want to do l2 normalization after every layer set to 'True'
l2_norm = False

#if you want to use dropout set to a value smaller than 1
dropout = 1

#Flag for using batch normalisation
batch_norm = False

#number of passes over the entire database
num_epochs = 10

#initial learning rate of the neural net
initial_learning_rate = 0.001

#exponential weight decay parameter
learning_rate_decay = 1

#size of the minibatch (#utterances)
batch_size = 128

#to limit memory ussage (specifically for GPU) the batch can be devided into
#even smaller batches. The gradient will be calculated by averaging the
#gradients of all these mini-batches. This value is the size of these
#mini-batches in number of utterances. For optimal speed this value should be
#set as high as possible without exeeding the memory. To use the entire batch
#set to -1
numutterances_per_minibatch = 16

#size of the validation set, set to 0 if you don't want to use one
valid_batches = 2

#frequency of evaluating the validation set
valid_frequency = 10

#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = True

#number of times the learning will retry (with half learning rate) before terminating the training
valid_retries = 3

#how many steps are taken between two checkpoints
check_freq = 10

#you can visualise the progress of the neural net with tensorboard
visualise = True
